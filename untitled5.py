# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V1MnHkYnft0U7XPY7pWaIF6F1L46HjJh
"""

# install libraries/packages/modules

!pip install -U git+https://github.com/albumentations-team/albumentations
!pip install timm
!pip install --upgrade opencv-contrib-python

# Download Dataset

!git clone https://github.com/parth1620/object-localization-dataset.git

import pandas as pd
import numpy as np
import cv2
import matplotlib.pyplot as plt
import torch
from tqdm.notebook import tqdm

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder



import sys
sys.path.append('/content/object-localization-dataset')

CSV_FILE= '/content/object-localization-dataset/train.csv'
DATA_DIR = '/content/object-localization-dataset/'

DEVICE = 'cuda'
BATCH_SIZE = 16
IMG_SIZE = 140

LR = 0.001
EPOCHS = 40
MODEL_NAME = 'efficientnet_b0'

NUM_COR = 4

df = pd.read_csv(CSV_FILE)
df

row = df.iloc[184]
img = cv2.imread(DATA_DIR + row.img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
pt1 = (row.xmin, row.ymin)
pt2= (row.xmax, row.ymax)
bnd_box_img = cv2.rectangle(img, pt1, pt2, (255,0,0), 2 )
plt.imshow(bnd_box_img)

train_df, valid_df = train_test_split(df, test_size = 0.20, random_state = 42)

import albumentations as A

train_augs = A.Compose([
        A.Resize(IMG_SIZE, IMG_SIZE),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p =0.5),
        A.Rotate()
], bbox_params=A.BboxParams(format = 'pascal_voc', label_fields =['class_labels']))

valid_augs = A.Compose([
        A.Resize(IMG_SIZE,IMG_SIZE)
], bbox_params=A.BboxParams(format = 'pascal_voc', label_fields =['class_labels']))

class ObjLocDataset(torch.utils.data.Dataset):

  def __init__(self, df, augmentations = None):
    self.df = df
    self.augmentation = augmentations

  def __len__(self):
    return len(self.df)

  def __getitem__(self, idx):

    row = self.df.iloc[idx]

    xmin = row.xmin
    ymin = row.ymin
    xmax = row.xmax
    ymax = row.ymax

    bbox = [[xmin, ymin, xmax, ymax]]
    img_path = DATA_DIR + row.img_path
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    if self.augmentation:
      data = self.augmentation(image = img, bboxes = bbox, class_labels = [None])
      img = data['image']
      bbox = data['bboxes'][0]

    img = torch.from_numpy(img).permute(2,0,1)/255.0 #(h,w,c)->(c,h,w)
    bbox = torch.tensor(bbox)

    return img, bbox

trainset = ObjLocDataset(train_df, train_augs)
validset = ObjLocDataset(valid_df, valid_augs)

print(f"Total examples in the trainset : {len(trainset)}")
print(f"Total examples in the validset : {len(validset)}")

img, bbox = trainset[6]


xmin, ymin, xmax, ymax = bbox

pt1 = (int(xmin), int(ymin))
pt2 = (int(xmax), int(ymax))

bnd_img = cv2.rectangle(img.permute(1,2,0).numpy(), pt1, pt2,(255,0,0), 2)

plt.imshow(bnd_img)
plt.axis('off')
plt.show

trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)
validloader = torch.utils.data.DataLoader(validset, batch_size = BATCH_SIZE, shuffle = False)

print(f"Total batches in the trainloader : {len(trainloader)}")
print(f"Total batches in the validloader : {len(validloader)}")

for images, bboxes in trainloader:
    break;

print(f"Images shape : {images.shape}")
print(f"Bboxes shape : {bboxes.shape}")

from torch import nn
import timm

class ObjLocModel(nn.Module):

    def __init__(self):
      super(ObjLocModel, self). __init__()

      self.backbone = timm.create_model(MODEL_NAME, pretrained = True, num_classes = 4)

    def forward(self, images, gt_bboxes = None):

      logits = self.backbone(images)
      bboxes = logits # Assuming logits are the predicted bounding boxes

      if gt_bboxes is not None:
        bboxes = bboxes.to(gt_bboxes.device) # Ensure bboxes are on the same device as gt_bboxes
        loss = nn.MSELoss()(bboxes, gt_bboxes)
        return bboxes, loss
      return bboxes

model = ObjLocModel()
model.to(DEVICE)

random_img = torch.rand(1,3,140,140).to(DEVICE)
model(random_img).shape

def train_fn(model, trainloader, criterion):
    total_loss = 0.0
    model.train()

    for data in tqdm(trainloader):
        images, gt_bboxes = data
        images, gt_bboxes = images.to(DEVICE), gt_bboxes.to(DEVICE)

        optimizer.zero_grad()
        bboxes, loss = model(images, gt_bboxes) # Model now returns bboxes and loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(trainloader)

def eval_fn(model, dataloader):
    total_loss = 0.0
    model.eval()

    with torch.no_grad():
         for data in tqdm(dataloader): # Changed to use the passed dataloader
            images, gt_bboxes = data
            images, gt_bboxes = images.to(DEVICE), gt_bboxes.to(DEVICE)


            bboxes, loss = model(images, gt_bboxes) # Model now returns bboxes and loss


            total_loss += loss.item()

    return total_loss / len(dataloader) # Changed to use the passed dataloader

optimizer = torch.optim.Adam(model.parameters(), lr = LR)

best_valid_loss = np.inf

for i in range(EPOCHS):

    train_loss = train_fn(model, trainloader, optimizer)
    valid_loss = eval_fn(model, validloader)

    if valid_loss < best_valid_loss:
      torch.save(model.state_dict(), 'best_model.pt')
      print("Saved Best Model")
      best_valid_loss = valid_loss

    print(f"Epoch : {i+1}")
    print(f"Train Loss : {train_loss}")

import utils

model.load_state_dict(torch.load('best_model.pt'))
model.eval()

with torch.no_grad():

    image, gt_bbox = validset[1]
    image = image.unsqueeze(0).to(DEVICE)
    out_bbox = model(image)

    utils.compare_plots(image, gt_bbox, out_bbox)